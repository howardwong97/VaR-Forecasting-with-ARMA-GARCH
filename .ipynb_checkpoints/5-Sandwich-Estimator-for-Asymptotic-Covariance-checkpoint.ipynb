{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandwich Estimator for Asymptotic Covariance\n",
    "\n",
    "## Back to the Basics\n",
    "\n",
    "### Likelihood for One Observation\n",
    "\n",
    "Suppose we have a single data point, $x$, that is distributed according to a probability density function $f_\\theta$. The likelihood function $f_\\theta(x)$ is thought of as a function of the parameter $\\theta$ for fixed $x$, rather than the other way around:\n",
    "\n",
    "\\begin{equation}\n",
    "    L_x(\\theta) = f_{\\theta}(x)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    l_x(\\theta) = \\log f_{\\theta}(x)\n",
    "\\end{equation}\n",
    "\n",
    "### Likelihood for iid Observations\n",
    "\n",
    "Suppose we have a sequence of $iid$ random variables, $X_1, X_2, \\dots, X_n$, that have a common probability density function, $f_{\\theta}$:\n",
    "\n",
    "\\begin{equation}\n",
    "    f_{n,\\theta} (\\mathbf{x}) = \\prod_{i=1}^n f_{\\theta}(x_i)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    l_n(\\theta) = \\sum_{i=1}^{n} \\log f_{\\theta}(x_i)\n",
    "\\end{equation}\n",
    "\n",
    "The value of the parameter $\\theta$ that maximises the log-likelihood function is called the *maximum likelihood estimate*, $\\hat{\\theta}_n$, where the subscript $n$ denotes $iid$ data.\n",
    "\n",
    "### Log Likelihood Derivatives\n",
    "\n",
    "Consider the first two derivatives, $l'_x$ and $l''_x$. Differentiating the identity,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\int f_{\\theta} (x) \\,dx = 1,\n",
    "\\end{equation}\n",
    "\n",
    "under the integral to get the following results:\n",
    "\n",
    "\\begin{equation}\n",
    "    E_{\\theta} \\{ l'_x (\\theta) \\} = 0\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    var_{\\theta} \\{ l'_x (\\theta) \\} = -E_{\\theta} \\{ l''_x (\\theta) \\}\n",
    "\\end{equation}\n",
    "\n",
    "### Fisher Information\n",
    "\n",
    "The Fisher Information, $I(\\theta)$ is defined with either side of Equation (7):\n",
    "\n",
    "\\begin{equation}\n",
    "    I(\\theta) = var_{\\theta} \\{ l'_x (\\theta) \\}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    I(\\theta) = -E_{\\theta} \\{ l''_x (\\theta) \\}.\n",
    "\\end{equation}\n",
    "\n",
    "This is a way of measuring the amount of *information* that an observable random variable $X$ carries about an unknown parameter $\\theta$ of a distribution that models $X$. Formally, it is the *variance of the score*, or the *expected value of the observed information*.\n",
    "\n",
    "The Fisher information matrix is used to calculate the covariance matrices associated with maximum-likelihood estimates.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    I_n(\\theta) &= -E_{\\theta} \\{ l''_n (\\theta) \\} \\\\\n",
    "    &=  -E_{\\theta} \\left\\{ \\frac{d^2}{d\\theta^2} \\sum_{i=1}^n \\log f_{\\theta} (x_i) \\right\\} \\\\\n",
    "    &= -\\sum_{i=1}^n  E_{\\theta} \\left\\{ \\frac{d^2}{d\\theta^2} \\log f_{\\theta} (x_i) \\right\\} \\\\\n",
    "    &= -\\sum_{i=1}^n  E_{\\theta} \\left\\{ l''_1(\\theta) \\right\\} \\\\\n",
    "    &= n I_1 (\\theta) \\\\\n",
    "    \\therefore I_n (\\theta) &= n I_1(\\theta).\n",
    "\\end{align}\n",
    "\n",
    "### Asymptotics of Log Likelihood Derivatives\n",
    "\n",
    "#### Law or Large Numbers\n",
    "\n",
    "With $iid$ data, the *law of large numbers* applies to any average,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{n} l'_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\frac{d}{d\\theta} \\log f_{\\theta}(x_i),\n",
    "\\end{equation}\n",
    "\n",
    "such that it will converge to its expectation, which as stated in Equation (6) should be equal to zero:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{n} l'_n(\\theta) \\xrightarrow{P} 0\n",
    "\\end{equation}\n",
    "\n",
    "Similarly, applying the law of large numbers to the average:\n",
    "\n",
    "\\begin{equation}\n",
    "    -\\frac{1}{n} l''_n(\\theta) = -\\frac{1}{n} \\sum_{i=1}^n \\frac{d^2}{d\\theta^2} \\log f_{\\theta}(x_i),\n",
    "\\end{equation}\n",
    "\n",
    "says this will converge to its expectation, which by Equation (15) is just $I_1 (\\theta)$. Thus,\n",
    "\n",
    "\\begin{equation}\n",
    "    -\\frac{1}{n} l''_n(\\theta) \\xrightarrow{P} I_1 (\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "#### Central Limit Theorem\n",
    "\n",
    "If $X_1, X_2, \\dots, X_n$ are random samples each of size $n$ taken from a population with overall mean $\\mu$ and finite variance $\\sigma^2$ and if $\\bar{X}$ is the sample mean, the limiting form of the distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "    Z = \\left( \\frac{\\bar{X}_n - \\mu}{\\sigma \\mathbin{/} \\sqrt{n}}\\right) \\quad \\text{as } n \\rightarrow \\infty,\n",
    "\\end{equation}\n",
    "\n",
    "is the *standard normal distribution*. If we apply this to $l'_n(\\theta)$,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{\\sqrt{n}} l'_n(\\theta) \\xrightarrow{D} N(0, I_1(\\theta))\n",
    "\\end{equation}\n",
    "\n",
    "where we notice that there is nothing to subtract because the expectation is equal to zero, and $\\sqrt{n} \\cdot \\left( \\frac{1}{n} \\right) = \\frac{1}{\\sqrt{n}}$\n",
    "\n",
    "### Asymptotics of MLE\n",
    "\n",
    "Assuming MLE is in the interior of the parameter space, the maximum log-likelihood occurs at:\n",
    "\n",
    "\\begin{equation}\n",
    "    l'_n (\\hat{\\theta}_n) = 0.\n",
    "\\end{equation}\n",
    "\n",
    "For large $n$, when $\\hat{\\theta}_n$ is close in value to $\\theta$ and, most importantly, assuming that it is a *consistent* estimator, $l'_n$ can be approximated by a Taylor series around $\\theta$:\n",
    "\n",
    "\\begin{equation}\n",
    "    l'_n (\\hat{\\theta}_n) \\approx l'_n(\\theta) + l''_n(\\theta)(\\hat{\\theta}_n - \\theta).\n",
    "\\end{equation}\n",
    "\n",
    "Since $l'_n (\\hat{\\theta}_n)$ is equal to zero, rearrange to find:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sqrt{n}(\\hat{\\theta}_n-\\theta) \\approx -\\frac{\\frac{1}{\\sqrt{n}}l'_n(\\theta)}{\\frac{1}{n}l''_n(\\theta)}.\n",
    "\\end{equation}\n",
    "\n",
    "According to the Central Limit Theorem, \n",
    "\n",
    "\\begin{equation}\n",
    "    -\\frac{\\frac{1}{\\sqrt{n}}l'_n(\\theta)}{\\frac{1}{n}l''_n(\\theta)} \\xrightarrow{D} \\frac{Z}{I_1(\\theta)}, \\quad \\text{where } Z \\sim N\\left(0, I_1(\\theta) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Now using the fact that $E(Z/c) = E(Z)/c$ and $var(Z/c) = var(Z)/c^2$, we get\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{Z}{I_1(\\theta)} \\sim N \\left( 0, I_1(\\theta)^{-1} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "and crucially,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sqrt{n} \\left( \\hat{\\theta}_n - \\theta \\right) \\xrightarrow{D} N \\left( 0, I_1(\\theta)^{-1} \\right).\n",
    "\\end{equation}\n",
    "\n",
    "### Observed Fisher Information\n",
    "\n",
    "The *observed Fisher information* is written as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{J}_n (\\theta) = -l''_n(\\theta).\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, Equation (27) can be rewritten as:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sqrt{I_n(\\theta)} \\cdot \\left( \\hat{\\theta}_n - \\theta \\right) \\xrightarrow{D} N \\left( 0, 1 \\right).\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sqrt{\\hat{J}_n (\\theta)} \\cdot \\left( \\hat{\\theta}_n - \\theta \\right) \\xrightarrow{D} N \\left( 0, 1 \\right).\n",
    "\\end{equation}\n",
    "\n",
    "## Misspecified Maximum Likelihood Estimation\n",
    "\n",
    "### Modifying the Theory under Model Misspecification\n",
    "\n",
    "The true distribution has no parameter $\\theta$ because it is not in the model. We now write $E_g$ and $var_g$. In addition, under model misspecification, for $E_\\theta$ and $var_\\theta$ must be the same in order for the differentiation under the integral sign to work (Equations 5, 6, 7). \n",
    "\n",
    "Consider the expectation of the log likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "    \\lambda_g (\\theta) = E_g \\{ l_X (\\theta) \\},\n",
    "\\end{equation}\n",
    "\n",
    "and suppose that the function $\\lambda_g$ achieves its maximum at some point $\\theta^*$. Assuming differentiation under the integral sign is possible,\n",
    "\n",
    "\\begin{equation}\n",
    "    E_g \\{ l'_X(\\theta^*) \\} = 0\n",
    "\\end{equation}\n",
    "\n",
    "Equation (7) is no longer valid under a misspecified model. The Fisher information is now defined with the two following equations:\n",
    "\n",
    "\\begin{equation}\n",
    "    V_n(\\theta) = var_g \\{l'_n(\\theta)\\} \\\\\n",
    "    J_n(\\theta) = -E_g \\{l''_n(\\theta)\\}\n",
    "\\end{equation}\n",
    "\n",
    "When the model is not misspecified, both $V_n$ and $J_n$ are simply equal to $I_n(\\theta)$. Similar to the result in Equation (15), we can now say that\n",
    "\n",
    "\\begin{equation}\n",
    "    V_n(\\theta) = n V_1(\\theta) \\\\\n",
    "    J_n(\\theta) = n J_1(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "### Asymptotics under Model Misspecification\n",
    "\n",
    "Following a similar process as in Section 4.1, we conclude that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sqrt{n} \\left( \\hat{\\theta}_n - \\theta^* \\right) \\xrightarrow{D} N \\left(0,J_1(\\theta^*)^{-1}V_1(\\theta^*)J_1(\\theta^*)^{-1})\\right),\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\theta}_n \\approx N \\left(\\theta^*,\\hat{J_n}(\\hat{\\theta}_n)^{-1}\\hat{V_n}(\\hat{\\theta}_n)\\hat{J_n}(\\hat{\\theta}_n)^{-1}\\right),\n",
    "\\end{equation}\n",
    "\n",
    "in which $V_n(\\theta)$ is replaced by an empirical estimate\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{V}_n(\\theta) = \\sum_{i=1}^{n} l'_n(\\theta)^2.\n",
    "\\end{equation}\n",
    "\n",
    "## The Sandwich Estimator\n",
    "\n",
    "The asymptotic variance here\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{J_n}(\\hat{\\theta}_n)^{-1}\\hat{V_n}(\\hat{\\theta}_n)\\hat{J_n}(\\hat{\\theta}_n)^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "is also called the *sandwich estimator*. Under model misspecification, the asymptotic variance is no longer simply the \"inverse Fisher information\". We calculate the asymptotic variance to allow us to compute t-stats for our estimates.\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute asymptotic variance:\n",
    "from numdifftools import Hessian, Jacobian\n",
    "\n",
    "jac = Jacobian(ag.get_loglikelihood)(estimates, p, q, X, True)\n",
    "V = jac.T.dot(jac)\n",
    "\n",
    "J = Hessian(ag.get_loglikelihood)(estimates, p, q, X, True)\n",
    "J_inv = np.linalg.inv(J)\n",
    "\n",
    "asymp_var = np.diag(J_inv.dot(V).dot(J_inv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "5",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
